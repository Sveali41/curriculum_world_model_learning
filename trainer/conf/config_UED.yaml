# 1. load the generator
training_generator:
  # data 
  map_width: 6
  map_height: 6
  data_dir: ${oc.env:GENERATOR_PATH}/data/dataset/generated_data_final.json
  batch_size: 1
  n_cpu: 0
  # model
  z_shape: 128
  generator: "deconv"
  n_epochs: 130
  lr: 1e-4
  wd : 0.0
  pth_path: ${oc.env:GENERATOR_PATH}/models/gan_model_classifer.ckpt #${oc.env:GENERATOR_PATH}/models/gan_model_classifier.ckpt
  validation_path: ${oc.env:GENERATOR_PATH}/models/gan_model_classifier.ckpt 
  map_element: {
        "wall": "W",
        "goal": "G",
        "door": "D",
        "key": "K",
        "lava": "L",
        "empty": "E",
        "floor": "F",
        "agent": "S",
        "ball": "B",
        "box": "X",
        "unseen": "?"
    } 
  color_element: {
    'R': 'red', 
    'G': 'green', 
    'B': 'blue',
    'Y': 'yellow',
    'M': 'magenta', 
    'C': 'cyan'
    }
  dynamic_objects: [8, 4, 5]
  # map_element: {'W': 0, 'E': 1, 'D': 2, 'G': 3, 'K':4 , 'Y': 5}
  env_path: ${oc.env:TRAINER_PATH}/level/level.txt
  use_wandb: False
  elites_path: ${oc.env:GENERATOR_PATH}/data/grid500_kd.pkl
  learning_buffer_size: 50 # 5000
  learning_buffer_threshold: 0.0001
  learning_steps: 200 # 1000
  

# 2.collect data from the generated env
env:
  # env_name: MiniGrid-Empty-8x8-v0 #MiniGrid-Dynamic-Obstacles-8x8-v0 # Name of environment, or comma-separate list of environment names to instantiate as each env in the VecEnv.
  visualize: False # to choose whether your want to see the env during interaction or not
  save_visualized_img: True
  visualize_save_path: ${oc.env:TRAINER_PATH}/logs/env_visualization/
  collect:
    env_type: norm # empty means empty only wall in preprocess no info for keys, lava/door/key/wall/norm
    data_type: random # random/ uniform -- data collection   random -- training
    episodes: 100  # 500 number of episodes to start
    maximum_dataset_size: None # maximum number of samples to collect, if no then None
    data_folder: ${oc.env:TRAINER_PATH}/data/ # folder where to save the rollouts
    data_save_path: ${oc.env:TRAINER_PATH}/data/gridworld_trainer.npz # folder where to save the rollouts
    num_workers: 0 # number of workers to use for data collection
    save_coverage_visualize: True
    visualize_save_path: ${oc.env:TRAINER_PATH}/logs/dataset_visualization/
    visualize_filename: "agent_coverage_uniform.png"
    ignore_done: true  # whether or not to ignore the done signal during data collection
    merge_subsets: True  # whether or not to merge subsets during data collection
    merge_k: 3  # number of subsets to merge if merge_subsets is True

# 3. attension model
attention_model:
  data: null
  env_type: empty # empty means empty only wall in preprocess no info for keys, lava/door/key/wall/norm
  data_type: discrete # 'norm' or 'discrete'  # 这两种类型效果都还不错 20250213
  model_type: Attention       # Attention, Embedding, MLP
  grid_shape: [3, 9, 10]  # Channel, Row, Col  # Row, Col 暂时不用，所以设为0就ok 20250213
  attention_mask_size: 3
  batch_size: 256  # test 64/128/256
  n_cpu: 8
  embed_dim: 128  # 128 64/128/256
  num_heads: 1   # 1 if the embed_dim is 64, num_heads should better be 1
  data_dir: ${oc.env:TRAINER_PATH}/data/gridworld_trainer.npz # ${oc.env:TRAINER_PATH}/data/gridworld_episode.npz 
  n_epochs: 25 # 25
  lr: 1e-3 # 1e-4 
  wd: 1e-5
  obs_norm_values: [10, 5, 3] # object, color, state --> agent_state: 1-down 2-left 3-up 0-right 
  action_norm_values: 6 # 0-left 1-right 2-forward 3-pickup 4-drop 5-toggle 6-done
  valid_values_obj: [1, 2, 4, 5, 8, 10]                                                             
  valid_values_color: [0, 1, 5]
  valid_values_state: [0, 1, 2, 3]
  use_wandb: False
  keep_cell_loss: false  # whether or not to keep cell loss
  n_phases_to_collect: 1  # how many phases to accumulate before training


  # freeze weights
  freeze_weight: False  # 置true 会加载权重，不会训练模型，用于验证
  # model_save_path: ${oc.env:MODEL_FPATH}/AttentionWM/attention_world_model.ckpt   # 这是整个模型的保存路径 20250213
  model_save_path: ${oc.env:MODEL_FPATH}/AttentionWM/attention_world_model.ckpt

  # plot
  visualization: False     
  visualize_every: 1000
  save_path: ${oc.env:TRAINER_PATH}/visulization
  direction_map: {0: 'right', 1: 'down', 2: 'left', 3: 'up'}
  action_map: {0: 'left', 1: 'right', 2: 'forward', 3: 'pickup', 4: 'drop', 5: 'toggle', 6: 'done'}

  # continual learning params
  continue_learning: true
  fisher_buffer_size: 500000
  current_sample_ratio: 0.3
  fisher_buffer_elements_ratio: 0.9 # proportion of key/door/lava samples in current fisher buffer sampling
  fisher_beta: 0.5 # fisher_beta 越大 → 越“快忘”旧任务。fisher_beta 越小 → 越“记得牢”旧任务。
  lambda_ewc: 300
  ewc_ratio: 0.4 # r 越大 → 越不忘旧任务，但越难学新任务。r 越小 → 越容易适应新任务，但更容易忘。
  lambda_ewc_min: 1e-4
  lambda_ewc_max: 1e3       
  lambda_ema: 0.05        # 让 λ 更新更平滑     
  fisher_samples: 2000     # how many samples to use for fisher information matrix computation    
  scale_factor: 1.0        # scale factor for fisher information matrix computation         
  drift_cooldown: 100
  
generator_agent:
  total_iterations: 50
  wm_train_frequency: 1
  batch_size: 16
  map_height: 8
  map_width: 8
  context_dim: 64
  max_edits: 0.3
  his_emb_dim: 16
  ratio: 0.25
